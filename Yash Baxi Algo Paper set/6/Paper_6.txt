--------------------
Question - 1(A)
--------------------

Big Theta (Î˜) notation is used to describe the asymptotic behavior of functions. It provides tight bounds on the growth rate of a function, both from above and below.

A function f(n) is said to be Î˜(g(n)) if and only if there exist positive constants ğ‘1,ğ‘2, and ğ‘›0 such that:

c1â‹…g(n)â‰¤f(n)â‰¤c2â‹…g(n)

for allğ‘› â‰¥ ğ‘›0.

Now, let's use this definition to show that 7ğ‘›2+8nâˆ’9=Î˜(n2).

We need to find constants ğ‘1,ğ‘2, and ğ‘›0such that:

c1.n2 <= 7n2 + 8n - 9 <= c2.n2 

for all ğ‘› â‰¥ ğ‘›0.

Let's first find an upper bound:

7ğ‘›2 + 8ğ‘› âˆ’ 9 â‰¤ 7ğ‘›2 + 8ğ‘›2 + 9ğ‘›2 
7ğ‘›2 + 8ğ‘› âˆ’ 9 â‰¤ 24ğ‘›2


So, we can choose 
ğ‘2 = 24 and ğ‘›0 = 1

Now, let's find a lower bound:

7ğ‘›2 + 8ğ‘› âˆ’ 9 â‰¥ 7ğ‘›2 âˆ’ 8ğ‘›2 âˆ’9ğ‘›2
 
7ğ‘›2 + 8ğ‘› âˆ’ 9 â‰¥ -10ğ‘›2 

So, we can choose ğ‘1 = 1 and ğ‘›0 = 1.

Therefore, 7ğ‘›2+8nâˆ’9=Î˜(n2).

--------------------
Question - 1(B)
--------------------

Sure, let's apply the Bubble Sort algorithm to sort the given list of numbers: 15 8 7 11 25 13 12 4.

Bubble Sort repeatedly steps through the list, compares adjacent elements, and swaps them if they are in the wrong order. This process continues until the list is sorted.

Step-by-step procedure:

1. **Initial List**: 15 8 7 11 25 13 12 4
2. **Pass 1**:
   - Compare 15 and 8: Swap (15 > 8)
   - Compare 15 and 7: Swap (15 > 7)
   - Compare 15 and 11: No Swap (15 < 11)
   - Compare 15 and 25: No Swap (15 < 25)
   - Compare 25 and 13: Swap (25 > 13)
   - Compare 25 and 12: Swap (25 > 12)
   - Compare 25 and 4: Swap (25 > 4)
   **List after Pass 1**: 8 7 11 15 13 12 4 25
   Number of exchanges in Pass 1: 5
   Number of comparisons in Pass 1: 7
3. **Pass 2**:
   - Compare 8 and 7: Swap (8 > 7)
   - Compare 8 and 11: No Swap (8 < 11)
   - Compare 11 and 15: No Swap (11 < 15)
   - Compare 15 and 13: Swap (15 > 13)
   - Compare 15 and 12: Swap (15 > 12)
   - Compare 15 and 4: Swap (15 > 4)
   **List after Pass 2**: 7 8 11 13 12 4 15 25
   Number of exchanges in Pass 2: 4
   Number of comparisons in Pass 2: 6
4. **Pass 3**:
   - Compare 7 and 8: No Swap (7 < 8)
   - Compare 8 and 11: No Swap (8 < 11)
   - Compare 11 and 13: No Swap (11 < 13)
   - Compare 13 and 15: No Swap (13 < 15)
   - Compare 15 and 12: Swap (15 > 12)
   - Compare 15 and 4: Swap (15 > 4)
   **List after Pass 3**: 7 8 11 12 4 13 15 25
   Number of exchanges in Pass 3: 2
   Number of comparisons in Pass 3: 5
5. **Pass 4**:
   - Compare 7 and 8: No Swap (7 < 8)
   - Compare 8 and 11: No Swap (8 < 11)
   - Compare 11 and 12: No Swap (11 < 12)
   - Compare 12 and 13: No Swap (12 < 13)
   - Compare 13 and 15: No Swap (13 < 15)
   - Compare 15 and 4: Swap (15 > 4)
   **List after Pass 4**: 7 8 11 12 4 13 15 25
   Number of exchanges in Pass 4: 1
   Number of comparisons in Pass 4: 5
6. **Pass 5**:
   - Compare 7 and 8: No Swap (7 < 8)
   - Compare 8 and 11: No Swap (8 < 11)
   - Compare 11 and 12: No Swap (11 < 12)
   - Compare 12 and 4: Swap (12 > 4)
   - Compare 12 and 13: No Swap (12 < 13)
   - Compare 13 and 15: No Swap (13 < 15)
   **List after Pass 5**: 7 8 11 4 12 13 15 25
   Number of exchanges in Pass 5: 1
   Number of comparisons in Pass 5: 5
7. **Pass 6**:
   - Compare 7 and 8: No Swap (7 < 8)
   - Compare 8 and 11: No Swap (8 < 11)
   - Compare 11 and 4: Swap (11 > 4)
   - Compare 11 and 12: No Swap (11 < 12)
   - Compare 12 and 13: No Swap (12 < 13)
   - Compare 13 and 15: No Swap (13 < 15)
   **List after Pass 6**: 7 8 4 11 12 13 15 25
   Number of exchanges in Pass 6: 1
   Number of comparisons in Pass 6: 5
8. **Pass 7**:
   - Compare 7 and 8: No Swap (7 < 8)
   - Compare 8 and 4: Swap (8 > 4)
   - Compare 8 and 11: No Swap (8 < 11)
   - Compare 11 and 12: No Swap (11 < 12)
   - Compare 12 and 13: No Swap (12 < 13)
   - Compare 13 and 15: No Swap (13 < 15)
   **List after Pass 7**: 7 4 8 11 12 13 15 25
   Number of exchanges in Pass 7: 1
   Number of comparisons in Pass 7: 5

9. **Pass 8**:
   - Compare 7 and 4: Swap (7 > 4)
   - Compare 7 and 8: No Swap (7 < 8)
   - Compare 8 and 11: No Swap (8 < 11)
   - Compare 11 and 12: No Swap (11 < 12)
   - Compare 12 and 13: No Swap (12 < 13)
   - Compare 13 and 15: No Swap (13 < 15)
   **List after Pass 8**: 4 7 8 11 12 13 15 25
   Number of exchanges in Pass 8: 1
   Number of comparisons in Pass 8: 5

10. **Pass 9**:
   - Compare 4 and 7: No Swap (4 < 7)
   - Compare 7 and 8: No Swap (7 < 8)
   - Compare 8 and 11: No Swap (8 < 11)
   - Compare 11 and 12: No Swap (11 < 12)
   - Compare 12 and 13: No Swap (12 < 13)
   - Compare 13 and 15: No Swap (13 < 15)
   **List after Pass 9**: 4 7 8 11 12 13 15 25
   Number of exchanges in Pass 9: 0
   Number of comparisons in Pass 9: 5

The sorted list after all passes of the Bubble


--------------------
Question - 1(C) ==> Send to yash Whatsapp
--------------------


--------------------
Question - 1(D)
--------------------

Since there's no graph provided, I'll illustrate three spanning trees of a weighted connected graph with random edge weights and vertices labeled as \(a\), \(b\), \(c\), and \(d\).

Let's denote the vertices as follows:
- \(a\)
- \(b\)
- \(c\)
- \(d\)

And suppose the edges have weights as shown in the diagram:

```
       7       8
   a ----- b ----- c
   |  6  / | 10  / |
   |    /  |    /  |  9
   |   /   |   /   |
   |  /    |  /    |
   | /     | /     |
   d ----- e ----- f
       12      9
```

Here are three spanning trees of this graph:

1. Spanning Tree 1:
```
   a ----- b ----- c
   |       |
   |  6    |  10
   |       |
   d ----- e ----- f
          9
```

2. Spanning Tree 2:
```
   a       b       c
           |       |
   d ----- e ----- f
       9
```

3. Spanning Tree 3:
```
   a ----- b
   |       |
   |       |
   d ----- e
``` 

These are three possible spanning trees of the given graph. Depending on the specific weights and connectivity of the graph, there could be other spanning trees as well.

--------------------
Question = 2(A)
--------------------

Sure, here are examples for each complexity class:

1. **( O(n) )**: Linear Time Complexity
   - Example: Finding the maximum element in an unsorted array.
     ```python
     def find_max(arr):
         max_val = arr[0]
         for num in arr:
             if num > max_val:
                 max_val = num
         return max_val
     ```
   - In this example, the time complexity is ( O(n) ) because the algorithm traverses the entire array once to find the maximum element.

2. **( O(n^2) )**: Quadratic Time Complexity
   - Example: Bubble Sort algorithm.
     ```python
     def bubble_sort(arr):
         n = len(arr)
         for i in range(n):
             for j in range(0, n-i-1):
                 if arr[j] > arr[j+1]:
                     arr[j], arr[j+1] = arr[j+1], arr[j]
     ```
   - In this example, the time complexity is \( O(n^2) \) because there are nested loops iterating over the array.

3. **( O(n log n) )**: Linearithmic Time Complexity
   - Example: Merge Sort algorithm.
     ```python
     def merge_sort(arr):
         if len(arr) > 1:
             mid = len(arr) // 2
             L = arr[:mid]
             R = arr[mid:]
             merge_sort(L)
             merge_sort(R)
             i = j = k = 0
             while i < len(L) and j < len(R):
                 if L[i] < R[j]:
                     arr[k] = L[i]
                     i += 1
                 else:
                     arr[k] = R[j]
                     j += 1
                 k += 1
             while i < len(L):
                 arr[k] = L[i]
                 i += 1
                 k += 1
             while j < len(R):
                 arr[k] = R[j]
                 j += 1
                 k += 1
     ```
   - In this example, the time complexity is ( O(n log n) ) because the merge sort algorithm divides the array into halves recursively and then merges them in ( O(n) ) time, resulting in ( O(n log n) ) overall.

--------------------
Question = 2(B - i)
--------------------

The Euclidean algorithm is an efficient method for computing the greatest common divisor (GCD) of two integers. It relies on the fact that the GCD of two numbers remains the same if the larger number is replaced by its difference with the smaller number.

Here's the Euclidean algorithm to compute the GCD of two non-negative integers ( a ) and ( b ):

1. If ( b = 0 ), return ( a ) as the GCD.
2. Otherwise, recursively compute the GCD of ( b ) and the remainder of ( a ) divided by ( b ).

Now, let's apply the Euclidean algorithm to find ( \text{GCD}(325, 95) ):

1.{GCD}(325, 95)
2.{GCD}(95, 325 mod 95)= {GCD}(95, 40)
3.{GCD}(40, 95 mod 40) = {GCD}(40, 15)
4.{GCD}(15, 40 mod 15) = {GCD}(15, 10)
5.{GCD}(10, 15 mod 10) = {GCD}(10, 5)
6.{GCD}(5, 10 mod 5)   = {GCD}(5, 0) 
Since ( b = 0 ), we return ( a = 5 ) as the GCD.

So, ( \text{GCD}(325, 95) = 5 ).

--------------------
Question = 2(B - ii)
--------------------

The complexity analysis of the Euclidean algorithm involves examining its time complexity in terms of the number of iterations needed to compute the greatest common divisor (GCD) of two non-negative integers (a) and (b).

Let (a) be the larger of the two numbers and (b) be the smaller one. In each step of the algorithm, the remainder of (a) divided by (b) (denoted (a mod b)) is computed. This operation takes (O(log(\max(a,b)))) time using efficient division algorithms. 

The algorithm continues until the remainder becomes zero, which occurs after at most (log(max(a,b))) iterations. 

Therefore, the time complexity of the Euclidean algorithm is (O(log(max(a,b)))), where (max(a,b)) represents the larger of the two input numbers (a) and (b). 

In terms of space complexity, the algorithm requires only a constant amount of additional space for storing intermediate values during each step, so the space complexity is (O(1)).

In summary:
- Time complexity: (O(\log(max(a,b)))\)
- Space complexity: (O(1))

--------------------
Question = 3(A)
--------------------

Kruskal's and Prim's algorithms are both popular methods for finding the minimum spanning tree (MST) of a connected, weighted graph, but they have different approaches and efficiencies.

1. **Approach**:
   - Kruskal's algorithm: It follows a greedy approach where it selects edges in ascending order of their weights and adds them to the MST if they don't form a cycle. It starts with the smallest edge and keeps adding edges until all vertices are included.
   - Prim's algorithm: It also follows a greedy approach but works by selecting vertices instead of edges. It starts with an arbitrary vertex and grows the MST by adding the cheapest edge that connects a vertex in the MST to a vertex outside the MST.

2. **Data Structures**:
   - Kruskal's algorithm typically uses a disjoint-set data structure (like Union-Find) to efficiently detect cycles and manage the connected components.
   - Prim's algorithm often uses a priority queue (such as a min-heap) to efficiently select the minimum weight edge or vertex to add to the MST.

3. **Complexity**:
   - Kruskal's algorithm has a time complexity of O(E log E) or O(E log V), where E is the number of edges and V is the number of vertices.
   - Prim's algorithm has a time complexity of O(V^2) with an adjacency matrix representation of the graph, or O(E + V log V) with an adjacency list representation, where E is the number of edges and V is the number of vertices.

4. **Applications**:
   - Kruskal's algorithm is often preferred when the graph is sparse (few edges) because it sorts edges based on weight.
   - Prim's algorithm can be more efficient on dense graphs (many edges) because it operates based on vertices and can take advantage of faster data structures for maintaining the priority queue.

5. **Parallelization**:
   - Kruskal's algorithm can be more easily parallelized because it operates on edges independently.
   - Prim's algorithm has dependencies between steps, making parallelization more challenging.

6. **Memory Usage**:
   - Kruskal's algorithm may require more memory due to the need to store edges and the disjoint-set data structure.
   - Prim's algorithm may require less memory as it operates directly on vertices and maintains a smaller priority queue.

In summary, both algorithms are efficient for finding MSTs, but the choice between them often depends on the characteristics of the graph and the available resources for implementation.


--------------------
Question = 3(B)
--------------------

Hand Written Given to Yash 


--------------------
Question = 4(A)
--------------------

Branch and Bound is a technique used in combinatorial optimization problems to systematically search through the space of possible solutions, efficiently pruning branches of the search tree that are guaranteed to be suboptimal. It combines the concepts of divide and conquer with greedy algorithm strategies.

Here's how Branch and Bound works:

1. **Branching**: The problem space is divided into smaller subspaces, often represented as a tree structure, where each node represents a partial solution or a decision point.
  
2. **Bounding**: At each node of the search tree, a bound (upper or lower) is calculated, which represents the best possible solution that can be obtained by exploring that node and its descendants.

3. **Pruning**: Nodes with bounds worse than the current best solution are pruned from further consideration, reducing the search space and improving efficiency.

4. **Backtracking**: The search proceeds recursively, exploring promising branches first. If a branch leads to a dead end or is pruned, the algorithm backtracks to the nearest decision point and explores an alternative branch.

Branch and Bound is particularly useful for optimization problems where:
- The search space is too large to be exhaustively explored.
- The problem has an objective function that can be used to evaluate the quality of partial solutions.
- There are constraints that limit the feasible solutions.

Example problem that can be solved using Branch and Bound:

**Traveling Salesman Problem (TSP)**:
Given a list of cities and the distances between them, the goal is to find the shortest possible route that visits each city exactly once and returns to the starting city. This problem is NP-hard and is typically solved using optimization techniques like Branch and Bound. In the Branch and Bound approach for TSP, at each node of the search tree, we branch by selecting the next city to visit, calculate a lower bound on the remaining distance, prune branches that exceed the current best solution, and backtrack when necessary. This process continues until all branches have been explored, and the optimal solution is found.

--------------------
Question = 4(B)
--------------------

To apply Dijkstra's algorithm to find the shortest paths from V1 to all other nodes, we'll use the following steps:

1. **Initialization**: Set the distance to V1 as 0 and all other distances to infinity. Keep track of the predecessor of each node.
2. **Selecting the Next Node**: Choose the node with the smallest tentative distance from V1.
3. **Updating Neighbors**: For each neighbor of the selected node, update its tentative distance from V1 if it can be reached with a shorter path.
4. **Marking Nodes as Visited**: Mark the selected node as visited to prevent revisiting it.
5. **Repeat Steps 2-4 Until All Nodes are Visited**: Continue selecting nodes and updating distances until all nodes have been visited.

Let's apply these steps:

Step 0 (Initialization):
- Set the distance to V1 as 0 and all other distances to infinity.
- Set the predecessor of each node to null.

| Node | Distance from V1 | Predecessor |
|------|------------------|-------------|
| V1   | 0                | Null        |
| V2   | âˆ                | Null        |
| V3   | âˆ                | Null        |
| V4   | âˆ                | Null        |
| V5   | âˆ                | Null        |

Step 1:
- Start from V1 (the source node) with a distance of 0.

Step 2:
- Select V1.

Step 3:
- Update distances to V2 and V3:
  - Distance to V2: 6 + 7 = 13
  - Distance to V3: 6 + 8 = 14

| Node | Distance from V1 | Predecessor |
|------|------------------|-------------|
| V1   | 0                | Null        |
| V2   | 13               | V1          |
| V3   | 14               | V1          |
| V4   | âˆ                | Null        |
| V5   | âˆ                | Null        |

Step 4:
- Mark V1 as visited.

Step 5:
- Select the unvisited node with the smallest distance (V2).

Step 6:
- Update distances to V3 and V5:
  - Distance to V3: 13 + 4 = 17
  - Distance to V5: 13 + âˆš6 = 13 + âˆš6

| Node | Distance from V1 | Predecessor |
|------|------------------|-------------|
| V1   | 0                | Null        |
| V2   | 13               | V1          |
| V3   | 14               | V1          |
| V4   | âˆ                | Null        |
| V5   | 13 + âˆš6          | V2          |

Step 7:
- Mark V2 as visited.

Step 8:
- Continue the process until all nodes are visited and distances are finalized.

This process will continue until all nodes are visited, and the shortest path from V1 to all other nodes will be determined. Each node will have its shortest distance from V1 and its predecessor in the shortest path.


--------------------
Question = 5(A)
--------------------

Certainly!

1. **Path**:
   - A path in a graph is a sequence of vertices where each adjacent pair of vertices is connected by an edge. 
   - Formally, a path (P) in a graph ( G ) is defined as an alternating sequence of vertices and edges: ( v_0, e_1, v_1, e_2, v_2, ..., e_{k-1}, v_{k-1}, e_k, v_k ), where ( e_i ) denotes the edge connecting ( v_{i-1} ) and ( v_i \) for ( 1 leq i leq k ).

2. **Cycle**:
   - A cycle in a graph is a path where the first and last vertices are the same, forming a closed loop.
   - Formally, a cycle ( C ) in a graph ( G ) is a path ( v_0, e_1, v_1, e_2, v_2, ..., e_{k-1}, v_{k-1}, e_k, v_k ) where ( v_0 = v_k ) and ( v_i ) are distinct for ( 0 leq i < k ).

3. **Complete Graph**:
   - A complete graph is a simple undirected graph where each pair of distinct vertices is connected by a unique edge.
   - Formally, a complete graph ( K_n ) is a graph with ( n ) vertices where every vertex is connected to every other vertex by an edge.
   - In a complete graph with ( n ) vertices, there are ( frac{n(n-1)}{2} ) edges, as each vertex is connected to ( n-1 ) other vertices.
   - Complete graphs are often denoted by the letter ( K ) followed by the number of vertices, such as (K) for a complete graph with 3 vertices.

In summary:
- A path is a sequence of vertices connected by edges, where no vertex appears more than once.
- A cycle is a closed path where the first and last vertices are the same, and no other vertex is repeated.
- A complete graph is a graph where every pair of distinct vertices is connected by an edge, forming a fully connected graph.


--------------------
Question = 5(B)
--------------------

Sure, here's a Python program to generate the Fibonacci series of 10 terms and count the number of loop iterations and assignment operations:

```
def fibonacci(n):
    fib_series = [0, 1]
    num_iterations = 0
    num_assignments = 2  # Assignments for initializing the list

    for i in range(2, n):
        fib_series.append(fib_series[i-1] + fib_series[i-2])
        num_iterations += 1
        num_assignments += 1  # Assignment for appending the next Fibonacci number

    return fib_series, num_iterations, num_assignments

n_terms = 10
series, iterations, assignments = fibonacci(n_terms)

print("Fibonacci series of {} terms:".format(n_terms))
print(series)

print("\nNumber of loop iterations:", iterations)
print("Number of assignment operations:", assignments)
```

This program defines a function `fibonacci()` that generates the Fibonacci series up to the specified number of terms (`n`). It iterates through the loop to generate the series and counts the number of iterations and assignment operations. Finally, it prints the Fibonacci series along with the counts.

When you run this program, it will output the Fibonacci series of 10 terms along with the number of loop iterations and assignment operations.